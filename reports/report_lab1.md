# Lexer & Scanner
## Course: Formal Languages & Finite Automata
## Author: Bzovîi Ana FAF-213





## Theory
    A formal language can be considered to be the media or the format used to convey information from a sender entity to the one that receives it. The usual components of a language are:

The alphabet: Set of valid characters;
The vocabulary: Set of valid words;
The grammar: Set of rules/constraints over the lang.
    Now these components can be established in an infinite amount of configurations, which actually means that whenever a language is being created, it's components should be selected in a way to make it as appropriate for it's use case as possible. Of course sometimes it is a matter of preference, that's why we ended up with lots of natural/programming/markup languages which might accomplish the same thing.

## Objectives:
- Understand what a language is and what it needs to have in order to be considered a formal one.
- Create a local && remote repository of a VCS hosting service;
- Choose a programming language, and my suggestion would be to choose one that supports all the main paradigms;
- Create a separate folder where you will be keeping the report;
- Implement a type/class for your grammar;
- Add one function that would generate 5 valid strings from the language expressed by your given grammar;
- Implement some functionality that would convert and object of type Grammar to one of type Finite Automaton;
- For the Finite Automaton, please add a method that checks if an input string can be obtained via the state transition from it;


  

## Implementation description
### Lexer class
The Lexer class in Python is responsible for parsing an input string and identifying the various types of tokens present in the string. To do this, the class defines a set of regular expression patterns that match different token types such as operators, identifiers, keywords, numbers, strings, and more. When the class's tokenize method is called, it iterates over these patterns, attempting to match them against the input string. If a match is found, the corresponding token and its value are added to a list of tokens. In case an invalid token is encountered, the class raises a Invalid character exception.
```python
class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __repr__(self):
        return f"Token(type='{self.type}', value={self.value})"

class Lexer:
    def __init__(self, text):
        self.text = text
        self.pos = 0
        self.current_char = self.text[self.pos]

    def advance(self):
        self.pos += 1
        if self.pos > len(self.text) - 1:
            self.current_char = None
        else:
            self.current_char = self.text[self.pos]

    def skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self.advance()

    def integer(self):
        result = ''
        while self.current_char is not None and self.current_char.isdigit():
            result += self.current_char
            self.advance()
        return int(result)

    def get_next_token(self):
        while self.current_char is not None:

            if self.current_char.isspace():
                self.skip_whitespace()
                continue

            if self.current_char.isdigit():
                return Token('INTEGER', self.integer())

            if self.current_char == '+':
                self.advance()
                return Token('PLUS', '+')

            if self.current_char == '-':
                self.advance()
                return Token('MINUS', '-')

            if self.current_char == '*':
                self.advance()
                return Token('MUL', '*')

            if self.current_char == '/':
                self.advance()
                return Token('DIV', '/')

            if self.current_char == '(':
                self.advance()
                return Token('LPAREN', '(')

            if self.current_char == ')':
                self.advance()
                return Token('RPAREN', ')')

            if self.current_char.isalpha():
                return Token('ID', self.current_char)

            raise Exception(f"Invalid character: {self.current_char}")

        return Token('EOF', None)


```
### Tokenize method
In a lexer file, a class token represents a specific category or type of lexeme that has been identified by the lexer. Tokens contain information such as the lexeme's type and any associated metadata, but do not necessarily retain its actual value. The token class is typically used to define various types of tokens that may appear in the input source code or document, such as keywords, identifiers, operators, or literals. These tokens are then generated by the lexer as it processes the input, and passed on to subsequent stages of the compiler or interpreter for further processing.

### Main
The Main class imports the Lexer class from the lexer module, and then creates an instance of the Lexer class by invoking its constructor without any arguments. Subsequently, the tokenize method of the Lexer instance is invoked with the input string "3 + 4 * 2 - 1". The method tokenizes the input string by separating it into a list of tokens and then returns the list. Finally, the resulting list of tokens is printed to the console using the print function. The output confirms that the input string has been correctly tokenized into its component tokens, including numbers, operators, and parentheses. Moreover, the message "input valid" is printed to the console, indicating that the input string was tokenized successfully without encountering any errors.```python
```python
 # Define input text
        text = "3 + 4 * 2 - 1"


        # Create lexer object
        lexer = Lexer(text)

        token = lexer.get_next_token()

        while token.type != "EOF":
            print(token)
            token = lexer.get_next_token()

        print(token)

```



## Results
Token(type='INTEGER', value=3)
Token(type='PLUS', value=+)
Token(type='INTEGER', value=4)
Token(type='MUL', value=*)
Token(type='INTEGER', value=2)
Token(type='MINUS', value=-)
Token(type='INTEGER', value=1)
Token(type='EOF', value=None)


## Conclusions
A lexer is a crucial component of programming language processing. It breaks down an input string into tokens that represent meaningful language elements. These tokens are mapped to corresponding types and semantics defined by production rules. The lexer is used for syntax highlighting, code completion, and program analysis. This project implements a lexer in Python using regular expressions to match token types. The implementation involves iterating over the input string, matching against regular expressions, and generating corresponding tokens. Regular expressions provide an efficient way to define the language's syntax. The project demonstrates the importance of lexers in language processing and shows how they can be used to build sophisticated compilers and analysis tools.
